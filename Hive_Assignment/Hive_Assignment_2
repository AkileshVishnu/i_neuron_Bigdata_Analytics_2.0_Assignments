Scenario Based questions:

Will the reducer work or not if you use “Limit 1” in any HiveQL query?
If you use the LIMIT clause with a value of 1 in a HiveQL query, it will not affect the functionality of the reducer. The reducer task will still be executed as per the underlying MapReduce framework. The LIMIT clause in Hive is applied after the reducer phase, so the reducer will process the entire input data set and perform any necessary aggregations or sorting before the limit is applied


-----------------------------------------------------------------------------------------------------------------------------------------------

Suppose I have installed Apache Hive on top of my Hadoop cluster using default metastore configuration. Then, what will happen if we have multiple clients trying to access Hive at the same time? 
If you have installed Apache Hive on your Hadoop cluster with the default metastore configuration, multiple clients accessing Hive at the same time will share the same metastore. The metastore is responsible for storing metadata about Hive tables, partitions, columns, and other objects.

When multiple clients access Hive simultaneously, they will interact with the metastore to perform operations such as creating tables, querying data, or altering table structures. The metastore is designed to handle concurrent access from multiple clients, ensuring data consistency and managing concurrent operations effectively.

-----------------------------------------------------------------------------------------------------------------------------------------------
Suppose, I create a table that contains details of all the transactions done by the customers: CREATE TABLE transaction_details (cust_id INT, amount FLOAT, month STRING, country STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘,’ ; Now, after inserting 50,000 records in this table, I want to know the total revenue generated for each month. But, Hive is taking too much time in processing this query. How will you solve this problem and list the steps that I will be taking in order to do so?

Some ways to Optimize the process in :
•	Enable Hive Tez Execution Engine
•	Create an ORC or Parquet Table
•	Partition the Table
•	Use Bucketing
•	Optimize query


-----------------------------------------------------------------------------------------------------------------------------------------------
How can you add a new partition for the month December in the above partitioned table? I am inserting data into a table based on partitions dynamically. But, I received an error – FAILED ERROR IN SEMANTIC ANALYSIS: Dynamic partition strict mode requires at least one static partition column. How will you remove this error?

Check the existing partition structure: Run the SHOW PARTITIONS command on your partitioned table to see the current partition structure. It will display the existing partitions and their corresponding column values.

Identify the static partition column: Determine which column(s) in your table's partition structure are static. Static partition columns have fixed values for all partitions. In your case, you need to identify the static partition column(s) to ensure you can add a new dynamic partition.

Alter table to add static partition column(s): If your table does not have any static partition columns, you need to alter the table structure to add a static partition column. You can do this using the ALTER TABLE statement with the ADD COLUMNS clause. For example, if you want to add a static partition column named year


Suppose, I have a CSV file – ‘sample.csv’ present in ‘/temp’ directory with the following entries: id first_name last_name email gender ip_address How will you consume this CSV file into the Hive warehouse using built-in SerDe?
CREATE EXTERNAL TABLE csv_table (
  id INT,
  first_name STRING,
  last_name STRING,
  email STRING,
  gender STRING,
  ip_address STRING
)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES (
  'separatorChar' = ',',
  'quoteChar' = '\"',
  'escapeChar' = '\\'
)
STORED AS TEXTFILE
LOCATION '/temp';


-----------------------------------------------------------------------------------------------------------------------------------------------
Suppose, I have a lot of small CSV files present in the input directory in HDFS and I want to create a single Hive table corresponding to these files. The data in these files are in the format: {id, name, e-mail, country}. Now, as we know, Hadoop performance degrades when we use lots of small files.So, how will you solve this problem where we want to create a single Hive table for lots of small files without degrading the performance of the system?

Combine small files: Instead of directly creating a Hive table for each small CSV file, merge the small files into larger files. This can be done using tools like Hadoop's distcp or using the getmerge command in Hadoop's File System shell. By combining the small files, you reduce the number of input files, which helps improve performance.

Convert to a larger file format: Once you have combined the small CSV files into larger files, convert them into a more efficient file format such as Apache Parquet or ORC (Optimized Row Columnar). These file formats are columnar and offer better compression and performance for querying.

Create an external table in Hive: Create an external table in Hive that corresponds to the merged and converted file(s). Define the table schema to match the CSV file's structure. You can use the appropriate SerDe (Serializer/Deserializer) for the file format you chose in the previous step.

-----------------------------------------------------------------------------------------------------------------------------------------------


Hive Practical questions:

Hive Join operations

Create a  table named CUSTOMERS(ID | NAME | AGE | ADDRESS   | SALARY)
Create a Second  table ORDER(OID | DATE | CUSTOMER_ID | AMOUNT
)

Now perform different joins operations on top of these tables
(Inner JOIN, LEFT OUTER JOIN ,RIGHT OUTER JOIN ,FULL OUTER JOIN)

BUILD A DATA PIPELINE WITH HIVE

Download a data from the given location - 
https://archive.ics.uci.edu/ml/machine-learning-databases/00360/

1. Create a hive table as per given schema in your dataset 
2. try to place a data into table location
3. Perform a select operation . 
4. Fetch the result of the select operation in your local as a csv file . 
5. Perform group by operation . 
7. Perform filter operation at least 5 kinds of filter examples . 
8. show and example of regex operation
9. alter table operation 
10 . drop table operation
12 . order by operation . 
13 . where clause operations you have to perform . 
14 . sorting operation you have to perform . 
15 . distinct operation you have to perform . 
16 . like an operation you have to perform . 
17 . union operation you have to perform . 
18 . table view operation you have to perform . 






Answers For Above:
1)
CREATE TABLE my_table (
    col1 INT,
    col2 STRING,
    col3 FLOAT
);

2) 
LOAD DATA INPATH '/path/to/data/file' INTO TABLE my_table;

3)
SELECT * FROM my_table;

4)
INSERT OVERWRITE LOCAL DIRECTORY '/path/to/local/directory'
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
SELECT * FROM my_table;

5)
SELECT col1, SUM(col3) AS total_sum
FROM my_table
GROUP BY col1;


6)
-- Example 1: Equal to condition
SELECT * FROM my_table WHERE col1 = 1;

-- Example 2: Not equal to condition
SELECT * FROM my_table WHERE col2 <> 'value';

-- Example 3: Greater than condition
SELECT * FROM my_table WHERE col3 > 10.0;

-- Example 4: Less than or equal to condition
SELECT * FROM my_table WHERE col1 <= 5;

-- Example 5: IN clause
SELECT * FROM my_table WHERE col2 IN ('value1', 'value2');



7) -- Extract rows where col2 matches a regex pattern
SELECT * FROM my_table WHERE col2 RLIKE 'pattern';

8) -- Add a new column to the table
ALTER TABLE my_table ADD COLUMN new_col INT;

9) -- Drop the table
DROP TABLE my_table;

10) -- Order the result by col1 in ascending order
SELECT * FROM my_table ORDER BY col1 ASC;

13) -- Retrieve distinct values of col1
SELECT DISTINCT col1 FROM my_table;


14) -- Retrieve rows where col2 matches a pattern
SELECT * FROM my_table WHERE col2 LIKE 'pattern%';

15) -- Retrieve rows where col2 matches a pattern
SELECT * FROM my_table WHERE col2 LIKE 'pattern%';

16) -- Retrieve rows where col2 matches a pattern
SELECT * FROM my_table WHERE col2 LIKE 'pattern%';





-----------------------------------------------------------------------------------------------------------------------------------------------




hive operation with python

Create a python application that connects to the Hive database for extracting data, creating sub tables for data processing, drops temporary tables.fetch rows to python itself into a list of tuples and mimic the join or filter operations

Below is the code for execution:

from pyhive import hive

# Establish a connection to Hive
conn = hive.Connection(host='localhost', port=10000, username='your_username')

# Create a cursor object
cursor = conn.cursor()

# Execute a Hive query to create a sub table
create_table_query = "CREATE TABLE sub_table AS SELECT * FROM main_table WHERE condition"
cursor.execute(create_table_query)

# Execute a Hive query to perform a join operation
join_query = "SELECT t1.column1, t2.column2 FROM table1 t1 JOIN table2 t2 ON t1.id = t2.id"
cursor.execute(join_query)

# Execute a Hive query to perform a filter operation
filter_query = "SELECT * FROM main_table WHERE column = 'value'"
cursor.execute(filter_query)

# Fetch rows from the result of the Hive query
rows = cursor.fetchall()

# Convert rows into a list of tuples
result = [tuple(row) for row in rows]

# Close the cursor and connection
cursor.close()
conn.close()

# Print the result
print(result)
